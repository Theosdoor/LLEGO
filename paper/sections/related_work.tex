\section{Related Work}

% =============================================================================
% PLANNING NOTES
% =============================================================================
%
% PARAGRAPH 1 - Decision Tree Induction
% - Classic methods: CART, C4.5, ID3
% - Optimal trees: DL8.5, GOSDT (MIP-based)
% - Evolutionary: genetic programming for trees
% - Recent: LLM-guided (LLEGO)
%
% PARAGRAPH 2 - LLMs for Optimization
% - EvoLLM: LLMs as crossover operators
% - OPRO: LLMs as optimizers
% - AutoML with LLMs
% - Common issue: cost of API calls
%
% PARAGRAPH 3 - Sparse Autoencoders & Mechanistic Interpretability
% - What are SAEs: unsupervised decomposition of activations
% - Monosemanticity: Anthropic's work on interpretable features
% - Dictionary learning perspective
% - Applications: steering, understanding, safety
%
% PARAGRAPH 4 - Knowledge Distillation
% - Teacher-student paradigm
% - Distilling LLM knowledge to smaller models
% - Our work: distilling semantic knowledge to a matrix
% - Novel: using SAEs for distillation
%
% KEY CITATIONS:
% - LLEGO: \cite{astorga2025autoformulation}
% - SAEs: Anthropic monosemanticity, Cunningham et al.
% - EvoLLM, OPRO
% - CART, DL8.5, GOSDT
%
% =============================================================================

\subsection{Decision Tree Induction}

% TODO: Background on tree methods

\subsection{LLMs for Optimization}

% TODO: EvoLLM, OPRO, etc.

\subsection{Mechanistic Interpretability and SAEs}

% TODO: SAE background

\subsection{Knowledge Distillation}

% TODO: Distillation context