\section{Discussion}

% =============================================================================
% PLANNING NOTES
% =============================================================================
%
% SECTION 5.1 - Summary of Contributions
% - First work combining SAEs with decision tree induction
% - Novel semantic coherence scoring
% - Zero-cost alternative to LLM-guided evolution
% - Interpretable priors (can inspect similarity matrix)
%
% SECTION 5.2 - When Does SAE-LLEGO Work Best?
% - Heart and credit: strong semantic relationships
% - Shallow depths: feature selection more critical
% - Domains with meaningful feature names
% - When SAE can extract good representations
%
% SECTION 5.3 - Limitations
% - Requires local GPU for extraction (but only once)
% - Depends on SAE quality and base model
% - Doesn't help on all datasets (liver underperforms)
% - Combined prior (Full) sometimes hurts - needs tuning
% - Feature names matter - can't work with opaque features
%
% SECTION 5.4 - Future Work
% - Adaptive weighting of structural vs semantic priors
% - Different SAE layers/sizes
% - Larger datasets with more features
% - Other domains: NLP, time series
% - Direct feature steering during evolution
%
% SECTION 5.5 - Broader Impact
% - Makes LLM-guided optimization more accessible
% - Reduces environmental cost (no repeated API calls)
% - Interpretability: can audit what semantic relationships are being used
%
% =============================================================================

\subsection{Summary}

% TODO: Summary paragraph

\subsection{When Does SAE-LLEGO Work Best?}

% TODO: Analysis of success conditions

\subsection{Limitations}

% TODO: Honest limitations

\subsection{Future Work}

% TODO: Extensions

\subsection{Conclusion}

% TODO: Final paragraph