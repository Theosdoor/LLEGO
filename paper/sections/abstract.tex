\begin{abstract}
% PLANNING NOTES:
% - Problem: LLEGO uses expensive LLM calls during evolution
% - Solution: Extract semantic priors ONCE using SAEs
% - Key results: Match/beat LLEGO on heart (+4%) and credit (+0.5%) at ZERO cost
% - Contribution: First work combining mechanistic interpretability with tree induction

% DRAFT:
Decision tree induction via LLM-guided evolution (LLEGO) achieves strong performance 
but requires expensive API calls during optimization. We propose \textbf{SAE-LLEGO}, 
which extracts semantic feature priors from LLM internals \emph{once} using Sparse 
Autoencoders, then reuses them throughout evolution at zero additional cost. On 
classification benchmarks, SAE-LLEGO matches or exceeds LLEGO performance on 2/4 
datasets while eliminating all LLM API calls. Our approach provides an interpretable, 
cost-effective alternative for distilling LLM knowledge into genetic algorithms.
\end{abstract}